---
title: "Assignment 3"
author: "Group 9. Rinus van Grunsven (10755373), Florens Douwes (11254483), Imad Lotfi (14651610)"
date: "xx October 2022"
output: pdf_document
fontsize: 11pt
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

### Exercise 3.1


### Exercise 3.2

The activity durations are all independent and exponentially distributed. Furthermore, the duration follows an exponential distribution with rate parameter 1/4. 
The project is calculated in excel as is seen in appendix Figure 1. The expected durations per activity are shown in the columns B16 until H16. As these are expected durations, we need to sample from this using the inverse transformation method. Excel's calculation for the inverse transformation method for exponential distribution is: '-ln(rand()) / “lambda”'. Lambda is the same the rate parameter so this will be 1/4. 
This function is used to calculate the realizations of activity durations for the seven activities in de cells B24 until H10023. So for example, the calculation made in cell B20 is '=-(LN(RAND())/(1/$B$16))'. Realizations of finish time activities is calculated in cells I24 untill O10023. This is done by adding the duration of the activity with the duration of the former one. So for example, in cell J20 it is '=I20+C20'. However, in cell M20 it is '=MAX(J20;K20)+F20'. This is because the activity five has to wait for both activity two as well as three to finish. The max makes sure that the longest of those two activities gets chosen as activity five can only start once both previous activities are finished. 

**a)** 

We can assume that the central limit theorem applies since the project has to be simulated 10000 times. The expected project finish time will be the average project finish time of those 10000 simulations. That calculation is shown in appendix Table 1 and the result is 12,089 (see appendix Figure 2).


**b)** 

The confidence interval is calculated using the t-test. The use of the t-test is possible as the we can assume a normal distribution (as described above), but don't know anything about the standard deviation. The t-distribution looks as follows:

<!-- $$ -->
<!-- \hat p - E < p < \hat p + E \text{ where }E = z_{a/2}\sqrt{\frac{\hat p \hat q}{n}} -->
<!-- $$ -->
<!-- where $p$ = population proportion, $\hat p$ = sample proportion, $n$ = number of sample values, $E$ = margin of error, and $z_{a/2}$ is the z-score. -->

The calulations are shown in appendix Table 1 and the results in appendix Figure 2. 




**c)** 




### Exercise 3.3

**a)** 
We can simulate the hotel room rent in excel by using multiple rows for random, binomial formulas. We arranged the data in a table (see appendix 3.3a table). With a simulation budget of 100000, we will use 100000 / 9 = 11111 rows.

When graphed (appendix 3.3b), we can see that the 140 mark is the cost with the highest estimate revenue.

We use a t-test for these calculations. The t-test is done in excel by estimating the mean, std. dev, and t value. From this, a lower- and upperbound confidence interval is calculated. These are also graphed in 3.3b. 

**b)**
To execute the steps in "ranking and selection, option 2" we can reuse the sheet created in 3.3a. With a starting budget of 900, and 9 prices, we first simulate 900 / 9 = 100 rows. These are shown in appendix 3.3c. 

Next, we perform pairwise t-test for each price. We can do this in Excel by creating a 9 by 9 table containing the test: $y(\pi) \le y(\pi') - \beta_{1-\alpha} \frac{\sqrt{s^2(\pi) + s^2(\pi')}}{\sqrt(N)}$. The results of these tests are in appendix 3.3d. With this, we can reject the prices 100, 100, 160, 170, 180.

To continue, we execute step 4, which is to perform the rest of the testing budget. This is 9100 / 9 = 1011 rows. The end result is shown in appendix 3.3e and 3.3f. Now, the maximum revenue is estimated to be with 130. 

An important note is that these values are generated by a random number generated, which resets between each simulation. Therefore, these test may show different results. Nonetheless, if simulated enough, the result should weight to a single price point eventually.

### Appendix

![Screenshot 3.2a](./exercise3,2a1.png) 
```{r echo=FALSE, message=FALSE, warning=FALSE}
my_tbl <- tibble::tribble(
       ~Name, ~Calclulation,
  "Sample average", '=AVERAGE(Q23:Q10022)',
  "N" , '=COUNT(Q23:Q10022)',
  "95 percentile of t-dist with n-1 " , '=T.INV(0,95;U28-1)',
  "Sample standard deviation (s)" , '=STDEV.S(Q23:Q1122)',
  "Left CI bound" , '=U25-U29*U30/SQRT(U28)',
  "Right CI bound" , '=U25+U29*U30/SQRT(U28)'
  )

require(knitr)
kable(my_tbl, digits = 3, row.names = FALSE, align = "l",
              caption = "Formulas used")
```



![Screenshot 3.2a2](./exercise3,2a2.png) 

![Screenshot 3.3a](./3.3a-table.png) 



![Screenshot 3.3b](./3.3a-graph.png) 

![Screenshot 3.3c](./3.3b-table.png) 

![Screenshot 3.3d](./3.3b-test.png) 
![Screenshot 3.3e](./3.3b-complete.png) 

![Screenshot 3.3f](./3.3b-graph.png) 




